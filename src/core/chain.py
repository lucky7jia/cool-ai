"""Analysis chain - orchestrates the multi-expert analysis process"""

import asyncio
from dataclasses import dataclass, field
from typing import Any, Optional
from datetime import datetime

from .expert import Expert, ExpertLoader
from .llm import get_llm_manager, LLMManager
from .plugin import get_plugin_manager, PluginManager

# Import stock data plugin
try:
    from plugins.data.stock import get_stock_context
    HAS_STOCK_PLUGIN = True
except ImportError:
    HAS_STOCK_PLUGIN = False
    async def get_stock_context(query: str) -> str:
        return ""


@dataclass
class SearchResult:
    """A single search result"""
    title: str
    url: str
    snippet: str
    content: Optional[str] = None


@dataclass
class ExpertAnalysis:
    """Analysis from a single expert"""
    expert_name: str
    expert_emoji: str
    analysis: str
    key_points: list[str] = field(default_factory=list)


@dataclass
class AnalysisResult:
    """Complete analysis result"""
    question: str
    search_results: list[SearchResult]
    expert_analyses: list[ExpertAnalysis]
    consensus: str
    iteration_count: int
    timestamp: datetime = field(default_factory=datetime.now)
    
    def to_markdown(self) -> str:
        """Convert result to markdown format"""
        lines = [
            f"# åˆ†ææŠ¥å‘Š",
            f"",
            f"**é—®é¢˜**: {self.question}",
            f"",
            f"**åˆ†ææ—¶é—´**: {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}",
            f"",
            f"**è¿­ä»£æ¬¡æ•°**: {self.iteration_count}",
            f"",
            f"---",
            f"",
            f"## ç»¼åˆç»“è®º",
            f"",
            self.consensus,
            f"",
            f"---",
            f"",
            f"## ä¸“å®¶åˆ†æ",
            f"",
        ]
        
        for analysis in self.expert_analyses:
            lines.extend([
                f"### {analysis.expert_emoji} {analysis.expert_name}",
                f"",
                analysis.analysis,
                f"",
            ])
        
        lines.extend([
            f"---",
            f"",
            f"## å‚è€ƒèµ„æ–™",
            f"",
        ])
        
        for i, result in enumerate(self.search_results[:5], 1):
            lines.append(f"{i}. [{result.title}]({result.url})")
        
        return "\n".join(lines)


class AnalysisChain:
    """Main analysis chain that orchestrates the multi-expert analysis"""
    
    def __init__(
        self,
        expert_loader: Optional[ExpertLoader] = None,
        llm_manager: Optional[LLMManager] = None,
        plugin_manager: Optional[PluginManager] = None,
        max_iterations: int = 3,
    ):
        self.expert_loader = expert_loader or ExpertLoader()
        self.llm_manager = llm_manager or get_llm_manager()
        self.plugin_manager = plugin_manager or get_plugin_manager()
        self.max_iterations = max_iterations
    
    async def search(self, query: str, important: bool = False) -> list[SearchResult]:
        """Search for information"""
        try:
            engine = "tavily" if important else None
            results = await self.plugin_manager.search(query, engine=engine)
            return [
                SearchResult(
                    title=r.get("title", ""),
                    url=r.get("url", ""),
                    snippet=r.get("snippet", ""),
                    content=r.get("content"),
                )
                for r in results
            ]
        except Exception as e:
            print(f"Search error: {e}")
            return []
    
    async def analyze_with_expert(
        self,
        expert: Expert,
        question: str,
        context: str,
    ) -> ExpertAnalysis:
        """Get analysis from a single expert"""
        analysis = await self.llm_manager.analyze_with_expert(
            question=question,
            expert_prompt=expert.system_prompt,
            context=context,
        )
        
        return ExpertAnalysis(
            expert_name=expert.name,
            expert_emoji=expert.metadata.emoji,
            analysis=analysis,
        )
    
    async def generate_consensus(
        self,
        question: str,
        analyses: list[ExpertAnalysis],
    ) -> str:
        """Generate a consensus from multiple expert analyses"""
        analyses_text = "\n\n".join([
            f"### {a.expert_emoji} {a.expert_name} çš„åˆ†æ:\n{a.analysis}"
            for a in analyses
        ])
        
        consensus_prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±çš„åˆ†ææ€»ç»“ä¸“å®¶ã€‚è¯·ç»¼åˆä»¥ä¸‹å¤šä½ä¸“å®¶çš„åˆ†æï¼Œç»™å‡ºæœ€ç»ˆçš„ç»¼åˆç»“è®ºã€‚

## åŸå§‹é—®é¢˜
{question}

## å„ä¸“å®¶åˆ†æ
{analyses_text}

## ç»¼åˆè¦æ±‚
1. æ‰¾å‡ºå„ä¸“å®¶è§‚ç‚¹çš„å…±è¯†ç‚¹
2. æŒ‡å‡ºå­˜åœ¨åˆ†æ­§çš„åœ°æ–¹å¹¶ç»™å‡ºåˆ¤æ–­
3. ç»¼åˆå½¢æˆæœ€ç»ˆç»“è®ºå’Œå»ºè®®
4. ç»™å‡ºé£é™©æç¤º

è¯·ç»™å‡ºä½ çš„ç»¼åˆç»“è®ºï¼š"""
        
        return await self.llm_manager.generate(consensus_prompt)
    
    async def run(
        self,
        question: str,
        expert_names: Optional[list[str]] = None,
        callback: Optional[callable] = None,
    ) -> AnalysisResult:
        """
        Run the full analysis chain.
        
        Args:
            question: The question to analyze
            expert_names: Optional list of specific experts to use
            callback: Optional callback for progress updates
        
        Returns:
            Complete analysis result
        """
        def log(msg: str):
            if callback:
                callback(msg)
            print(msg)
        
        # 1. Load experts
        log("ğŸ“š åŠ è½½ä¸“å®¶...")
        if expert_names:
            experts = [
                self.expert_loader.get_expert(name)
                for name in expert_names
                if self.expert_loader.get_expert(name)
            ]
        else:
            experts = self.expert_loader.find_relevant_experts(question)
        
        if not experts:
            raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„ä¸“å®¶")
        
        log(f"âœ… å·²åŠ è½½ {len(experts)} ä½ä¸“å®¶: {', '.join(e.get_display_name() for e in experts)}")
        
        # 2. Get real-time stock data if available
        stock_context = ""
        if HAS_STOCK_PLUGIN:
            log("ğŸ“ˆ è·å–å®æ—¶è‚¡ç¥¨æ•°æ®...")
            try:
                stock_context = await get_stock_context(question)
                if stock_context:
                    log("âœ… å·²è·å–å®æ—¶è¡Œæƒ…æ•°æ®")
                else:
                    log("â„¹ï¸ æœªè¯†åˆ«åˆ°è‚¡ç¥¨ä»£ç ")
            except Exception as e:
                log(f"âš ï¸ è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {e}")
        
        # 3. Web search
        log("ğŸ” æœç´¢ç›¸å…³ä¿¡æ¯...")
        search_results = await self.search(question)
        search_context = "\n\n".join([
            f"**{r.title}**\n{r.snippet}"
            for r in search_results[:5]
        ])
        log(f"âœ… æ‰¾åˆ° {len(search_results)} æ¡ç›¸å…³ä¿¡æ¯")
        
        # Combine all context
        context = ""
        if stock_context:
            context += f"## ğŸ“Š å®æ—¶è¡Œæƒ…æ•°æ®\n\n{stock_context}\n\n"
        if search_context:
            context += f"## ğŸ” æœç´¢ç»“æœ\n\n{search_context}"
        if not context:
            context = "æš‚æ— é¢å¤–èƒŒæ™¯ä¿¡æ¯"
        
        # 3. Multi-expert analysis with iterations
        all_analyses: list[ExpertAnalysis] = []
        
        for iteration in range(self.max_iterations):
            log(f"\nğŸ”„ ç¬¬ {iteration + 1}/{self.max_iterations} è½®åˆ†æ...")
            
            # Run expert analyses in parallel
            tasks = [
                self.analyze_with_expert(expert, question, context)
                for expert in experts
            ]
            analyses = await asyncio.gather(*tasks)
            all_analyses = list(analyses)
            
            for analysis in all_analyses:
                log(f"  {analysis.expert_emoji} {analysis.expert_name} å®Œæˆåˆ†æ")
            
            # Check if we need more iterations (simplified logic)
            if iteration < self.max_iterations - 1:
                # Search for additional information based on analyses
                log("ğŸ” è¡¥å……æœç´¢...")
                supplement_query = f"{question} è¯¦ç»†åˆ†æ æœ€æ–°æ•°æ®"
                new_results = await self.search(supplement_query, important=True)
                if new_results:
                    search_results.extend(new_results[:3])
                    context += "\n\n" + "\n\n".join([
                        f"**{r.title}**\n{r.snippet}"
                        for r in new_results[:3]
                    ])
        
        # 4. Generate consensus
        log("\nğŸ“ ç”Ÿæˆç»¼åˆç»“è®º...")
        consensus = await self.generate_consensus(question, all_analyses)
        
        log("âœ… åˆ†æå®Œæˆ!")
        
        return AnalysisResult(
            question=question,
            search_results=search_results,
            expert_analyses=all_analyses,
            consensus=consensus,
            iteration_count=self.max_iterations,
        )

"""Iteration and self-verification system"""

import asyncio
import json
from dataclasses import dataclass
from typing import Callable, Optional

from .chain import AnalysisChain, AnalysisResult, ExpertAnalysis
from .config import get_config
from .llm import get_llm_manager


@dataclass
class IterationResult:
    """Result of an iteration cycle"""
    iteration_number: int
    result: AnalysisResult
    gaps_identified: list[str]
    supplemental_queries: list[str]
    consensus_score: float


class IterativeAnalyzer:
    """
    Iterative analysis with self-verification.
    
    The key insight: local models may have limited capability,
    but multiple iterations can compensate by:
    1. Cross-validating expert opinions
    2. Identifying information gaps
    3. Supplementing with additional searches
    4. Refining conclusions
    """

    def __init__(
        self,
        chain: AnalysisChain,
        max_iterations: Optional[int] = None,
        consensus_threshold: float = 0.8,
        on_progress: Optional[Callable[[str], None]] = None,
    ):
        config = get_config()
        self.chain = chain
        self.max_iterations = max_iterations or config.max_iterations
        self.consensus_threshold = consensus_threshold
        self.on_progress = on_progress or (lambda x: None)

    def _report_progress(self, message: str) -> None:
        """Report progress to callback"""
        self.on_progress(message)

    async def _identify_gaps(
        self,
        query: str,
        analyses: list[ExpertAnalysis],
    ) -> tuple[list[str], list[str]]:
        """
        Identify information gaps and generate supplemental queries.
        
        Returns:
            Tuple of (gaps, supplemental_queries)
        """
        llm = get_llm_manager()
        
        # Build a prompt to identify gaps
        analyses_text = "\n\n".join([
            f"**{a.expert_name}**: {a.analysis[:500]}..."
            for a in analyses
        ])

        gap_prompt = """ä½ æ˜¯ä¸€ä½ç ”ç©¶æ–¹æ³•ä¸“å®¶ï¼Œè´Ÿè´£è¯†åˆ«åˆ†æä¸­çš„ä¿¡æ¯ç¼ºå£ã€‚

è¯·åˆ†æå„ä¸“å®¶çš„æ„è§ï¼Œæ‰¾å‡ºï¼š
1. å“ªäº›è§‚ç‚¹å­˜åœ¨åˆ†æ­§ï¼Ÿ
2. å“ªäº›ä¿¡æ¯è¿˜ä¸å¤Ÿå……åˆ†ï¼Ÿ
3. éœ€è¦è¡¥å……æœç´¢ä»€ä¹ˆå†…å®¹ï¼Ÿ

è¯·ç”¨JSONæ ¼å¼è¾“å‡ºï¼š
{
  "gaps": ["ç¼ºå£1", "ç¼ºå£2"],
  "queries": ["è¡¥å……æœç´¢1", "è¡¥å……æœç´¢2"]
}"""

        gap_query = f"""åŸå§‹é—®é¢˜ï¼š{query}

å„ä¸“å®¶åˆ†æï¼š
{analyses_text}

è¯·è¯†åˆ«ä¿¡æ¯ç¼ºå£å’Œéœ€è¦è¡¥å……çš„æœç´¢ã€‚"""

        result = await llm.generate(gap_query, system_prompt=gap_prompt)

        # Simple parsing
        gaps = []
        queries = []
        
        try:
            # Try to find JSON in the response
            start = result.find("{")
            end = result.rfind("}") + 1
            if start >= 0 and end > start:
                data = json.loads(result[start:end])
                gaps = data.get("gaps", [])
                queries = data.get("queries", [])
        except Exception:
            # If parsing fails, use the raw text as a gap
            gaps = [result[:200]]
            queries = []

        return gaps, queries

    async def _calculate_consensus(
        self,
        analyses: list[ExpertAnalysis],
    ) -> float:
        """
        Calculate consensus score among experts.
        
        Returns a score between 0 and 1.
        """
        if len(analyses) < 2:
            return 1.0

        # Simple heuristic: check for keywords indicating agreement/disagreement
        agreement_keywords = ["åŒæ„", "ä¸€è‡´", "ç›¸ä¼¼", "æ”¯æŒ", "è®¤åŒ", "agree", "similar"]
        disagreement_keywords = ["ä¸åŒæ„", "åˆ†æ­§", "åå¯¹", "è´¨ç–‘", "disagree", "differ"]

        total_agreement = 0
        total_disagreement = 0

        for analysis in analyses:
            text = analysis.analysis.lower()
            for kw in agreement_keywords:
                total_agreement += text.count(kw)
            for kw in disagreement_keywords:
                total_disagreement += text.count(kw)

        if total_agreement + total_disagreement == 0:
            return 0.7  # Default moderate consensus

        score = total_agreement / (total_agreement + total_disagreement + 1)
        return min(1.0, max(0.0, 0.5 + score * 0.5))

    async def run(
        self,
        query: str,
        experts: Optional[list[str]] = None,
    ) -> AnalysisResult:
        """
        Run iterative analysis with self-verification.
        
        Args:
            query: User's question
            experts: Optional list of expert names
            
        Returns:
            Final analysis result after iterations
        """
        self._report_progress(f"ğŸ”„ å¼€å§‹è¿­ä»£åˆ†æ (æœ€å¤š {self.max_iterations} è½®)...")

        current_query = query
        all_search_results = []
        iteration = 0
        result = None

        while iteration < self.max_iterations:
            iteration += 1
            self._report_progress(f"\nğŸ“Š ç¬¬ {iteration} è½®è¿­ä»£...")

            # Run analysis chain
            result = await self.chain.run(
                question=current_query,
                expert_names=experts,
                callback=self._report_progress,
            )

            # Accumulate search results
            all_search_results.extend(result.search_results)

            # Calculate consensus
            consensus_score = await self._calculate_consensus(result.expert_analyses)
            self._report_progress(f"  ğŸ“ˆ å…±è¯†åº¦: {consensus_score:.1%}")

            # Check if consensus is reached
            if consensus_score >= self.consensus_threshold:
                self._report_progress(f"âœ… è¾¾æˆå…±è¯†ï¼Œç»“æŸè¿­ä»£")
                result.iteration_count = iteration
                result.search_results = list(set(all_search_results))
                return result

            # Identify gaps for next iteration
            if iteration < self.max_iterations:
                self._report_progress("  ğŸ” è¯†åˆ«ä¿¡æ¯ç¼ºå£...")
                gaps, supplemental_queries = await self._identify_gaps(
                    query, result.expert_analyses
                )

                if not supplemental_queries:
                    self._report_progress("  â„¹ï¸ æ²¡æœ‰æ–°çš„æœç´¢å»ºè®®ï¼Œç»“æŸè¿­ä»£")
                    result.iteration_count = iteration
                    result.search_results = list(set(all_search_results))
                    return result

                # Update query for next iteration
                current_query = f"{query}\n\nè¡¥å……ä¿¡æ¯éœ€æ±‚ï¼š{'; '.join(supplemental_queries[:2])}"
                self._report_progress(f"  ğŸ“ è¡¥å……æœç´¢: {supplemental_queries[:2]}")

        self._report_progress(f"â¹ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° ({self.max_iterations})")
        if result:
            result.iteration_count = iteration
            result.search_results = list(set(all_search_results))
        return result
